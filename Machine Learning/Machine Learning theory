Supervised Learning Models
Supervised learning uses labeled training data where each input comes with a corresponding correct output. The model learns to map inputs to outputs by analyzing these
examples. This approach is fundamental to many machine learning applications and can be divided into two main categories: classification and regression
Classification Models
Classification models predict discrete categorical outcomes, like determining whether an email is spam or not spam
Logistic Regression: is one of the most fundamental classification algorithms. Despite its name, it's used for classification rather than regression. It uses the logistic function to
model the probability of binary outcomes (yes/no, true/false). The model outputs probabilities between 0 and 1, making it int erpretable for decision-making scenarios.
Support Vector Machines (SVM): work by finding the optimal hyperplane that best separates data points into different classes. SVMs are particularly powerful because they
can handle both linearly and non-linearly separable data through kernel functions. The Linear Kernel is used when data is linearly separable, represented as K(x,y) = x·y.
The Polynomial Kernel allows modeling of more complex relationships using K(x,y) = (x·y + c)^d, where c is a constant and d is the polynomial degree. The Radial Basis
Function (RBF) Kernel is the most widely used kernel, mapping data into infinite-dimensional space to handle complex, non-linear relationships.
Decision Trees: use a tree-like structure where each internal node represents a test on a feature, and each branch represents the outcome of that test. They're highly
interpretable but can be prone to overfitting. Each path from root to leaf represents a decision rule.
Random Forest: is an ensemble method that constructs multiple decision trees independently on random subsets of data and features. The final prediction is made by
averaging (regression) or majority voting (classification) across all trees. This approach reduces overfitting and improves accuracy compared to individual decision trees.
k-Nearest Neighbors (kNN):classifies new data points based on the majority class of their k nearest neighbors in the feature space. It's a lazy learning algorithm that doesn't
build an explicit model during training but rather stores all training examples.
Naive Bayes :applies Bayes' theorem with the "naive" assumption that features are independent of each other. Despite this strong assumption, it often performs well in practice,
especially for text classification tasks like spam detection.
Linear Regression: describes the relationship between dependent and independent variables using a linear 
